update.x<-function(k,mu, logsig)
{
rtnorm(1, mean=mu, sd=exp(logsig), lower= y[k]-0.5, upper=y[k]+0.5)
}
update.mu<-function(xbar, logsig)
{
rnorm(1, mean=xbar, sd= exp(logsig)/sqrt(10))
}
update.logsig<-function(x, xbar, mu)
{
#b<-sum((x-mu)^2)
b<- sum((x-xbar)^2) + 10*(xbar-mu)^2
1/rgamma(1, shape=4, scale=b/2)
}
#run mcmc
N.iter<-2000
#creating a posterior matrix
post<-matrix(0,ncol=12,nrow=N.iter)
colnames(post)<-c("x1","x2","x3", "x4","x5","x6","x7","x8","x9","x10", "mu","logsig")
post[1,]<-c(6.6, 6.4,7.3, 4.7, 5.3, 2.6, 5.6, 5.1, 4.4, 3.3, 2.3, 0.5)
for(i in 2:N.iter)
{
for(k in 1:10)
{
post[i, k ]<-update.x(k,mu=post[i-1,"mu"], logsig=post[i-1,"logsig"])
}
x<-c(post[i,1], post[i,2], post[i,3], post[i,4], post[i,5], post[i,6], post[i,7], post[i,8], post[i,9], post[i,10])
xbar<-mean(x)
post[i,"mu"]<-update.mu(xbar, logsig=post[i-1,"logsig"])
post[i,"logsig"]<-update.logsig(x,xbar, post[i,"mu"])
}
sig2<-summ(exp(post[,"logsig"]));sig2
#--------------------------------------
# Calculate posterior summaries:
#--------------------------------------
# Actually, I am not really interested in (mu, logsig) but the parameters
# (mu, sigma2), where sigma = exp(logsigma)^2
# We get samples for these parameters as follows:
mu.sig.gibbs <- cbind(post[,'x1'], post[,'x2'], post[,'x3'], post[,'x4'], post[,'x5'], post[,'x6'], post[,'x7'], post[,'x8'], post[,'x9'], post[,'x10'], post[, 'mu'], post[, 'logsig'])
# ... cbind() makes a matrix out of column vectors
# Set meaningful names to the columns of matrix mu.sig.gibbs:
colnames(mu.sig.gibbs) <- c('x1','x2','x3','x4','x5','x6','x7','x8','x9','x10', 'mu', 'logsigma')
# Let's reject first half of the simulations
mu.sig.gibbs <- mu.sig.gibbs[-(1 : round(N.iter/2) ), ]
# Posterior means:
apply(mu.sig.gibbs, 2, mean)
# This is a fancy way of calculating
# mean(mu.sig.gibbs[,1]) and mean(mu.sig.gibbs[,2])
# with a single call.
# Posterior covariance:
cov(mu.sig.gibbs)
# Quantiles of the marginal posterior distributions:
apply(mu.sig.gibbs, 2, quantile, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# You can obtain the same information with two calls:
# quantile(mu.sig.gibbs[,1], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# quantile(mu.sig.gibbs[,2], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
gibbs.mcmc <- mcmc(mu.sig.gibbs)
par(mfrow=c(2,2))
plot(gibbs.mcmc)
autocorr.plot(gibbs.mcmc)
effectiveSize(gibbs.mcmc)
###########
mu.sig.gibbs1 <- cbind(post[, 'mu'], exp(post[, 'logsig'])^2)
# ... cbind() makes a matrix out of column vectors
# Set meaningful names to the columns of matrix mu.sig.gibbs:
colnames(mu.sig.gibbs1) <- c('mu', 'sigma2')
# Let's reject first half of the simulations
mu.sig.gibbs1 <- mu.sig.gibbs1[-(1 : round(N.iter/2) ), ]
# Posterior means:
apply(mu.sig.gibbs1, 2, mean)
# This is a fancy way of calculating
# mean(mu.sig.gibbs[,1]) and mean(mu.sig.gibbs[,2])
# with a single call.
# Quantiles of the marginal posterior distributions:
apply(mu.sig.gibbs1, 2, quantile, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# You can obtain the same information with two calls:
# quantile(mu.sig.gibbs[,1], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# quantile(mu.sig.gibbs[,2], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
autocorr.plot(gibbs.mcmc1)
effectiveSize(gibbs.mcmc1)
#----------------------
# Visualize results
#----------------------
#MODEL CHECKS --- DIC
mu.bayes = mean(post[,"mu"])
logsig.bayes = mean(post[,"logsig"])
x1.bayes = mean(post[,"x1"])
x2.bayes = mean(post[,"x2"])
x3.bayes = mean(post[,"x3"])
x4.bayes = mean(post[,"x4"])
x5.bayes = mean(post[,"x5"])
x6.bayes = mean(post[,"x6"])
x7.bayes = mean(post[,"x7"])
x8.bayes = mean(post[,"x8"])
x9.bayes = mean(post[,"x9"])
x10.bayes = mean(post[,"x10"])
x.bayes=c(x1.bayes,x2.bayes,x3.bayes,x4.bayes,x5.bayes,x6.bayes,x7.bayes,x8.bayes,x9.bayes,x10.bayes)
#find logP(yi|mu,sig,xi) using posterior samples.
log.likelihood = NULL
update.likelihood<-function(x, mu, sd)
{
sum(dnorm(x, mean = mu, sd = sd, log = TRUE))
}
#logP(y|theta_bayes)
log.like.bayes<-update.likelihood(x.bayes,mu.bayes, exp(logsig.bayes) )
#logP(y|theta) at every posterior samples
for(i in 1:N.iter)
{
x<-c(post[i,1], post[i,2], post[i,3], post[i,4], post[i,5], post[i,6], post[i,7], post[i,8], post[i,9], post[i,10])
mu<-post[i,"mu"]
logsig<-post[i,"logsig"]
log.likelihood[i]<-update.likelihood(x, mu, exp(logsig))
}
BIC= -2*log.like.bayes+2*log(N.iter);BIC
AIC= -2*log.like.bayes+2*2;AIC
#P_DIC
p.DIC = 2*(log.like.bayes-mean(log.likelihood));p.DIC
#ELPD_DIC
elpd.DIC = log.like.bayes-p.DIC;elpd.DIC
#DIC
DIC = -2*elpd.DIC+2*p.DIC;DIC
#P.DIC alternative is 2*var(log(p(y|theta.post)))
p.DIC.alt = 2*var(log.likelihood);p.DIC.alt
DIC.alt = -2*log.like.bayes+2*p.DIC.alt;DIC.alt
# Make the contour plot and add the simulated points.
contour (mugrid, logsdgrid, dens, levels=contours, labex=0, cex=2)
points(post[, 'mu'], post[, 'logsig'], pch = '.')
title(main = 'Gibbs sampler', xlab = 'mu', ylab = 'logsig')
y<-c(7,6,7,5,5,3,6,5,4,3)
n <- length(y)
update.x<-function(k,mu, logsig)
{
rtnorm(1, mean=mu, sd=exp(logsig), lower= y[k]-0.5, upper=y[k]+0.5)
}
update.mu<-function(xbar, logsig)
{
rnorm(1, mean=xbar, sd= exp(logsig)/sqrt(10))
}
update.logsig<-function(x, xbar, mu)
{
#b<-sum((x-mu)^2)
b<- sum((x-xbar)^2) + 10*(xbar-mu)^2
1/rgamma(1, shape=4, scale=b/2)
}
#run mcmc
N.iter<-2000
#creating a posterior matrix
post<-matrix(0,ncol=12,nrow=N.iter)
colnames(post)<-c("x1","x2","x3", "x4","x5","x6","x7","x8","x9","x10", "mu","logsig")
post[1,]<-c(6.6, 6.4,7.3, 4.7, 5.3, 2.6, 5.6, 5.1, 4.4, 3.3, 2.3, 0.5)
for(i in 2:N.iter)
{
for(k in 1:10)
{
post[i, k ]<-update.x(k,mu=post[i-1,"mu"], logsig=post[i-1,"logsig"])
}
x<-c(post[i,1], post[i,2], post[i,3], post[i,4], post[i,5], post[i,6], post[i,7], post[i,8], post[i,9], post[i,10])
xbar<-mean(x)
post[i,"mu"]<-update.mu(xbar, logsig=post[i-1,"logsig"])
post[i,"logsig"]<-update.logsig(x,xbar, post[i,"mu"])
}
sig2<-summ(exp(post[,"logsig"]));sig2
#--------------------------------------
# Calculate posterior summaries:
#--------------------------------------
# Actually, I am not really interested in (mu, logsig) but the parameters
# (mu, sigma2), where sigma = exp(logsigma)^2
# We get samples for these parameters as follows:
mu.sig.gibbs <- cbind(post[,'x1'], post[,'x2'], post[,'x3'], post[,'x4'], post[,'x5'], post[,'x6'], post[,'x7'], post[,'x8'], post[,'x9'], post[,'x10'], post[, 'mu'], post[, 'logsig'])
# ... cbind() makes a matrix out of column vectors
# Set meaningful names to the columns of matrix mu.sig.gibbs:
colnames(mu.sig.gibbs) <- c('x1','x2','x3','x4','x5','x6','x7','x8','x9','x10', 'mu', 'logsigma')
# Let's reject first half of the simulations
mu.sig.gibbs <- mu.sig.gibbs[-(1 : round(N.iter/2) ), ]
# Posterior means:
apply(mu.sig.gibbs, 2, mean)
# This is a fancy way of calculating
# mean(mu.sig.gibbs[,1]) and mean(mu.sig.gibbs[,2])
# with a single call.
# Posterior covariance:
cov(mu.sig.gibbs)
# Quantiles of the marginal posterior distributions:
apply(mu.sig.gibbs, 2, quantile, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# You can obtain the same information with two calls:
# quantile(mu.sig.gibbs[,1], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# quantile(mu.sig.gibbs[,2], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
gibbs.mcmc <- mcmc(mu.sig.gibbs)
par(mfrow=c(2,2))
plot(gibbs.mcmc)
autocorr.plot(gibbs.mcmc)
effectiveSize(gibbs.mcmc)
###########
mu.sig.gibbs1 <- cbind(post[, 'mu'], exp(post[, 'logsig'])^2)
# ... cbind() makes a matrix out of column vectors
# Set meaningful names to the columns of matrix mu.sig.gibbs:
colnames(mu.sig.gibbs1) <- c('mu', 'sigma2')
# Let's reject first half of the simulations
mu.sig.gibbs1 <- mu.sig.gibbs1[-(1 : round(N.iter/2) ), ]
# Posterior means:
apply(mu.sig.gibbs1, 2, mean)
# This is a fancy way of calculating
# mean(mu.sig.gibbs[,1]) and mean(mu.sig.gibbs[,2])
# with a single call.
# Quantiles of the marginal posterior distributions:
apply(mu.sig.gibbs1, 2, quantile, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# You can obtain the same information with two calls:
# quantile(mu.sig.gibbs[,1], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# quantile(mu.sig.gibbs[,2], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
autocorr.plot(gibbs.mcmc1)
effectiveSize(gibbs.mcmc1)
#----------------------
# Visualize results
#----------------------
#MODEL CHECKS --- DIC
mu.bayes = mean(post[,"mu"])
logsig.bayes = mean(post[,"logsig"])
x1.bayes = mean(post[,"x1"])
x2.bayes = mean(post[,"x2"])
x3.bayes = mean(post[,"x3"])
x4.bayes = mean(post[,"x4"])
x5.bayes = mean(post[,"x5"])
x6.bayes = mean(post[,"x6"])
x7.bayes = mean(post[,"x7"])
x8.bayes = mean(post[,"x8"])
x9.bayes = mean(post[,"x9"])
x10.bayes = mean(post[,"x10"])
x.bayes=c(x1.bayes,x2.bayes,x3.bayes,x4.bayes,x5.bayes,x6.bayes,x7.bayes,x8.bayes,x9.bayes,x10.bayes)
#find logP(yi|mu,sig,xi) using posterior samples.
log.likelihood = NULL
update.likelihood<-function(x, mu, sd)
{
sum(dnorm(x, mean = mu, sd = sd, log = TRUE))
}
#logP(y|theta_bayes)
log.like.bayes<-update.likelihood(x.bayes,mu.bayes, exp(logsig.bayes) )
#logP(y|theta) at every posterior samples
for(i in 1:N.iter)
{
x<-c(post[i,1], post[i,2], post[i,3], post[i,4], post[i,5], post[i,6], post[i,7], post[i,8], post[i,9], post[i,10])
mu<-post[i,"mu"]
logsig<-post[i,"logsig"]
log.likelihood[i]<-update.likelihood(x, mu, exp(logsig))
}
BIC= -2*log.like.bayes+2*log(N.iter);BIC
AIC= -2*log.like.bayes+2*2;AIC
#P_DIC
p.DIC = 2*(log.like.bayes-mean(log.likelihood));p.DIC
#ELPD_DIC
elpd.DIC = log.like.bayes-p.DIC;elpd.DIC
#DIC
DIC = -2*elpd.DIC+2*p.DIC;DIC
#P.DIC alternative is 2*var(log(p(y|theta.post)))
p.DIC.alt = 2*var(log.likelihood);p.DIC.alt
DIC.alt = -2*log.like.bayes+2*p.DIC.alt;DIC.alt
#part2
y<-c(7,6,7,5,5,3,6,5,4,3)
n <- length(y)
update.x<-function(k,mu, logsig)
{
rtnorm(1, mean=mu, sd=exp(logsig), lower= y[k]-0.5, upper=y[k]+0.5)
}
update.mu<-function(xbar, logsig)
{
rnorm(1, mean=xbar, sd= exp(logsig)/sqrt(10))
}
update.logsig<-function(x, xbar, mu)
{
#b<-sum((x-mu)^2)
b<- sum((x-xbar)^2) + 10*(xbar-mu)^2
1/rgamma(1, shape=4, scale=b/2)
}
#run mcmc
N.iter<-2000
#creating a posterior matrix
post<-matrix(0,ncol=12,nrow=N.iter)
colnames(post)<-c("x1","x2","x3", "x4","x5","x6","x7","x8","x9","x10", "mu","logsig")
post[1,]<-c(6.6, 6.4,7.3, 4.7, 5.3, 2.6, 5.6, 5.1, 4.4, 3.3, 2.3, 0.5)
for(i in 2:N.iter)
{
for(k in 1:10)
{
post[i, k ]<-update.x(k,mu=post[i-1,"mu"], logsig=post[i-1,"logsig"])
}
x<-c(post[i,1], post[i,2], post[i,3], post[i,4], post[i,5], post[i,6], post[i,7], post[i,8], post[i,9], post[i,10])
xbar<-mean(x)
post[i,"mu"]<-update.mu(xbar, logsig=post[i-1,"logsig"])
post[i,"logsig"]<-update.logsig(x,xbar, post[i,"mu"])
}
sig2<-summ(exp(post[,"logsig"]));sig2
#--------------------------------------
# Calculate posterior summaries:
#--------------------------------------
# Actually, I am not really interested in (mu, logsig) but the parameters
# (mu, sigma2), where sigma = exp(logsigma)^2
# We get samples for these parameters as follows:
mu.sig.gibbs <- cbind(post[,'x1'], post[,'x2'], post[,'x3'], post[,'x4'], post[,'x5'], post[,'x6'], post[,'x7'], post[,'x8'], post[,'x9'], post[,'x10'], post[, 'mu'], post[, 'logsig'])
# ... cbind() makes a matrix out of column vectors
# Set meaningful names to the columns of matrix mu.sig.gibbs:
colnames(mu.sig.gibbs) <- c('x1','x2','x3','x4','x5','x6','x7','x8','x9','x10', 'mu', 'logsigma')
# Let's reject first half of the simulations
mu.sig.gibbs <- mu.sig.gibbs[-(1 : round(N.iter/2) ), ]
# Posterior means:
apply(mu.sig.gibbs, 2, mean)
# This is a fancy way of calculating
# mean(mu.sig.gibbs[,1]) and mean(mu.sig.gibbs[,2])
# with a single call.
# Posterior covariance:
cov(mu.sig.gibbs)
# Quantiles of the marginal posterior distributions:
apply(mu.sig.gibbs, 2, quantile, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# You can obtain the same information with two calls:
# quantile(mu.sig.gibbs[,1], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# quantile(mu.sig.gibbs[,2], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
gibbs.mcmc <- mcmc(mu.sig.gibbs)
par(mfrow=c(2,2))
plot(gibbs.mcmc)
autocorr.plot(gibbs.mcmc)
effectiveSize(gibbs.mcmc)
###########
mu.sig.gibbs1 <- cbind(post[, 'mu'], exp(post[, 'logsig'])^2)
# ... cbind() makes a matrix out of column vectors
# Set meaningful names to the columns of matrix mu.sig.gibbs:
colnames(mu.sig.gibbs1) <- c('mu', 'sigma2')
# Let's reject first half of the simulations
mu.sig.gibbs1 <- mu.sig.gibbs1[-(1 : round(N.iter/2) ), ]
# Posterior means:
apply(mu.sig.gibbs1, 2, mean)
# This is a fancy way of calculating
# mean(mu.sig.gibbs[,1]) and mean(mu.sig.gibbs[,2])
# with a single call.
# Quantiles of the marginal posterior distributions:
apply(mu.sig.gibbs1, 2, quantile, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# You can obtain the same information with two calls:
# quantile(mu.sig.gibbs[,1], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# quantile(mu.sig.gibbs[,2], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
autocorr.plot(gibbs.mcmc1)
effectiveSize(gibbs.mcmc1)
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
autocorr.plot(gibbs.mcmc1)
effectiveSize(gibbs.mcmc1)
library("msm")
library("MCMCpack")
library(coda)
y<-c(7,6,7,5,5,3,6,5,4,3)
n <- length(y)
update.x<-function(k,mu, logsig)
{
rtnorm(1, mean=mu, sd=exp(logsig), lower= y[k]-0.5, upper=y[k]+0.5)
}
update.mu<-function(xbar, logsig)
{
rnorm(1, mean=xbar, sd= exp(logsig)/sqrt(10))
}
update.logsig<-function(x, xbar, mu)
{
#b<-sum((x-mu)^2)
b<- sum((x-xbar)^2) + 10*(xbar-mu)^2
1/rgamma(1, shape=4, scale=b/2)
}
#run mcmc
N.iter<-2000
#creating a posterior matrix
post<-matrix(0,ncol=12,nrow=N.iter)
colnames(post)<-c("x1","x2","x3", "x4","x5","x6","x7","x8","x9","x10", "mu","logsig")
post[1,]<-c(6.6, 6.4,7.3, 4.7, 5.3, 2.6, 5.6, 5.1, 4.4, 3.3, 2.3, 0.5)
for(i in 2:N.iter)
{
for(k in 1:10)
{
post[i, k ]<-update.x(k,mu=post[i-1,"mu"], logsig=post[i-1,"logsig"])
}
x<-c(post[i,1], post[i,2], post[i,3], post[i,4], post[i,5], post[i,6], post[i,7], post[i,8], post[i,9], post[i,10])
xbar<-mean(x)
post[i,"mu"]<-update.mu(xbar, logsig=post[i-1,"logsig"])
post[i,"logsig"]<-update.logsig(x,xbar, post[i,"mu"])
}
sig2<-summ(exp(post[,"logsig"]));sig2
#--------------------------------------
# Calculate posterior summaries:
#--------------------------------------
# Actually, I am not really interested in (mu, logsig) but the parameters
# (mu, sigma2), where sigma = exp(logsigma)^2
# We get samples for these parameters as follows:
mu.sig.gibbs <- cbind(post[,'x1'], post[,'x2'], post[,'x3'], post[,'x4'], post[,'x5'], post[,'x6'], post[,'x7'], post[,'x8'], post[,'x9'], post[,'x10'], post[, 'mu'], post[, 'logsig'])
# ... cbind() makes a matrix out of column vectors
# Set meaningful names to the columns of matrix mu.sig.gibbs:
colnames(mu.sig.gibbs) <- c('x1','x2','x3','x4','x5','x6','x7','x8','x9','x10', 'mu', 'logsigma')
# Let's reject first half of the simulations
mu.sig.gibbs <- mu.sig.gibbs[-(1 : round(N.iter/2) ), ]
# Posterior means:
apply(mu.sig.gibbs, 2, mean)
# This is a fancy way of calculating
# mean(mu.sig.gibbs[,1]) and mean(mu.sig.gibbs[,2])
# with a single call.
# Posterior covariance:
cov(mu.sig.gibbs)
# Quantiles of the marginal posterior distributions:
apply(mu.sig.gibbs, 2, quantile, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# You can obtain the same information with two calls:
# quantile(mu.sig.gibbs[,1], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# quantile(mu.sig.gibbs[,2], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
gibbs.mcmc <- mcmc(mu.sig.gibbs)
par(mfrow=c(2,2))
plot(gibbs.mcmc)
###########
mu.sig.gibbs1 <- cbind(post[, 'mu'], exp(post[, 'logsig'])^2)
# ... cbind() makes a matrix out of column vectors
# Set meaningful names to the columns of matrix mu.sig.gibbs:
colnames(mu.sig.gibbs1) <- c('mu', 'sigma2')
# Let's reject first half of the simulations
mu.sig.gibbs1 <- mu.sig.gibbs1[-(1 : round(N.iter/2) ), ]
# Posterior means:
apply(mu.sig.gibbs1, 2, mean)
# This is a fancy way of calculating
# mean(mu.sig.gibbs[,1]) and mean(mu.sig.gibbs[,2])
# with a single call.
# Quantiles of the marginal posterior distributions:
apply(mu.sig.gibbs1, 2, quantile, probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# You can obtain the same information with two calls:
# quantile(mu.sig.gibbs[,1], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
# quantile(mu.sig.gibbs[,2], probs = c(0.025, 0.25, 0.5, 0.75, 0.975))
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
autocorr.plot(gibbs.mcmc1)
effectiveSize(gibbs.mcmc1)
#----------------------
# Visualize results
#----------------------
#MODEL CHECKS --- DIC
mu.bayes = mean(post[,"mu"])
logsig.bayes = mean(post[,"logsig"])
x1.bayes = mean(post[,"x1"])
x2.bayes = mean(post[,"x2"])
x3.bayes = mean(post[,"x3"])
x4.bayes = mean(post[,"x4"])
x5.bayes = mean(post[,"x5"])
x6.bayes = mean(post[,"x6"])
x7.bayes = mean(post[,"x7"])
x8.bayes = mean(post[,"x8"])
x9.bayes = mean(post[,"x9"])
x10.bayes = mean(post[,"x10"])
x.bayes=c(x1.bayes,x2.bayes,x3.bayes,x4.bayes,x5.bayes,x6.bayes,x7.bayes,x8.bayes,x9.bayes,x10.bayes)
#find logP(yi|mu,sig,xi) using posterior samples.
log.likelihood = NULL
update.likelihood<-function(x, mu, sd)
{
sum(dnorm(x, mean = mu, sd = sd, log = TRUE))
}
#logP(y|theta_bayes)
log.like.bayes<-update.likelihood(x.bayes,mu.bayes, exp(logsig.bayes) )
#logP(y|theta) at every posterior samples
for(i in 1:N.iter)
{
x<-c(post[i,1], post[i,2], post[i,3], post[i,4], post[i,5], post[i,6], post[i,7], post[i,8], post[i,9], post[i,10])
mu<-post[i,"mu"]
logsig<-post[i,"logsig"]
log.likelihood[i]<-update.likelihood(x, mu, exp(logsig))
}
BIC= -2*log.like.bayes+2*log(N.iter);BIC
AIC= -2*log.like.bayes+2*2;AIC
#P_DIC
p.DIC = 2*(log.like.bayes-mean(log.likelihood));p.DIC
#ELPD_DIC
elpd.DIC = log.like.bayes-p.DIC;elpd.DIC
#DIC
DIC = -2*elpd.DIC+2*p.DIC;DIC
#P.DIC alternative is 2*var(log(p(y|theta.post)))
p.DIC.alt = 2*var(log.likelihood);p.DIC.alt
DIC.alt = -2*log.like.bayes+2*p.DIC.alt;DIC.alt
# Make the contour plot and add the simulated points.
#contour (mugrid, logsdgrid, dens, levels=contours, labex=0, cex=2)
#points(post[, 'mu'], post[, 'logsig'], pch = '.')
#title(main = 'Gibbs sampler', xlab = 'mu', ylab = 'logsig')
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
gibbs.mcmc1 <- mcmc(mu.sig.gibbs1)
par(mfrow=c(2,2))
plot(gibbs.mcmc1)
effectiveSize(gibbs.mcmc)
autocorr.plot(gibbs.mcmc)
acf(mu.sig.gibbs[,2])
acf(mu.sig.gibbs[,2],lag.max = 50)
acf(mu.sig.gibbs[,7],lag.max = 50)
lapply(mu.sig.gibbs,effectiveSize)
lapply(gibbs.mcmc,effectiveSize)
lapply(gibbs.mcmc[,1],effectiveSize)
lapply(gibbs.mcmc[1],effectiveSize)
gibbs.mcmc <- mcmc(mu.sig.gibbs)
par(mfrow=c(2,2))
plot(gibbs.mcmc)
